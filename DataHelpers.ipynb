{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marttiylikoski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm # show progress bar for long running ops\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#algorithm based on knowing that in the data there are just two labels 0=normal tweet, 1=disaster tweet\n",
    "def plot_LSA(test_data, test_labels, plot=True):\n",
    "    lsa = TruncatedSVD(n_components=2)\n",
    "    lsa.fit(test_data)\n",
    "    lsa_scores = lsa.transform(test_data)\n",
    "    color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "    color_column = [color_mapper[label] for label in test_labels]\n",
    "    colors = ['orange','blue','blue']\n",
    "    if plot:\n",
    "        plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "        red_patch = mpatches.Patch(color='orange', label='Normal')\n",
    "        green_patch = mpatches.Patch(color='blue', label='Disaster')\n",
    "        plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n",
    "    \n",
    "\n",
    "def get_pretrained_glove_vectors(file):\n",
    "    embedding_dict={}\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word = values[0]\n",
    "            vectors=np.asarray(values[1:],'float32')\n",
    "            embedding_dict[word]=vectors\n",
    "        f.close()\n",
    "    \n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "def create_corpus_new(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usage:\n",
    "# Showing Confusion Matrix for BERT model\n",
    "#plot_confision_matrix(train_pred_BERT_int, train['target'].values, 'Confusion matrix for BERT model', figsize=(7,7))\n",
    "\n",
    "# Showing Confusion Matrix\n",
    "def plot_confusion_matrixy(y_true, y_pred, title, figsize=(5,5)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d\\n\\n\\n\\n' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d\\n\\n\\n\\n' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'True label'\n",
    "    cm.columns.name = 'Predicted label'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "#    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "#    ax.set_title('Confusion Matrix');    \n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix\n",
    "# https://likegeeks.com/seaborn-heatmap-tutorial/\n",
    "\n",
    "def plot_cm(y_true, y_pred, title, figsize=(5,5)):\n",
    "    df = pd.DataFrame({'y_Actual':y_true, 'y_Predicted':y_pred})\n",
    "    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = False)\n",
    "    \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(confusion_matrix, annot=True,  ax = ax)\n",
    "    sns.set(font_scale=1.5)\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['disaster', 'normal']); ax.yaxis.set_ticklabels(['normal', 'disaster']);\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
